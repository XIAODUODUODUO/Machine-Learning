# KNN算法原理
- K近邻是一种基本的机器学习算法，K就是K个最近邻的意思。每个样本都可以用它最接近的K个邻居来代表。可用于分类应用和回归应用。
- KNN在做回归和分类的主要区别在于最后做预测的时候决策方式不同。
  - 回归预测时：一般采用平均值法
  - 分类预测时：一般采用多数表决法
- 从训练集合中获取K个离待预测样本距离很近的样本数据
- 根据获取得到的K歌样本数据来预测当前待预测样本的目标属性值

# KNN 算法的三个重要因素
- K值的选择
  - 对于K值的选择，一般根据样本分布选择较小的值，然后通过交叉验证来选择一个比较合适的最终值；
  - 当选择比较小的K值的时候，表示使用较小领域中的样本进行预测，训练误差会减少，但是会导致模型变得复杂，容易过拟合；
    - K值选择过小：训练集的准确率100%，测试集的准确率不一定好。过拟合模型
  - 当选择比较大的K值的时候，表示使用较大领域中的样本进行预测，训练误差会增大，但是模型会变得简单，容易欠拟合。
    - K值选择过大：f(x)=1 常数函数，欠拟合模型

- 距离度量
  - 欧式距离

- 决策规则
  - 分类模型
    - 多数表决法：每个邻近样本的权重是一样的，也就是说最终预测的结果为出现类别最多的那个类
    - 加权多数表决法：每个邻近样本的权重不一样，一般情况下采用权重和距离成反比的方式来计算，也就是说最终预测结果是出现权重最大的那个类别
    
  - 回归模型
    - 平均值法：每个邻近样本的权重是一样的，最终预测结果为所有邻近样本的平均值
    - 加权平均值法：每个邻近样本的权重不一样，一般情况下采用权重和距离成反比的方式来计算，也就是说在计算均值的时候进行加权操作
- 总结：
  - K值的选择：过小，容易过拟合；过大，容易欠拟合
  - 距离度量：欧式距离
  - 决策规则：多数表决法，加权多数表决法；平均值法，加权平均值法

# KNN 算法的实现
  - KNN加权投票--分类 【KNN加权算法.py】
    - 初始化数据
    - 初始化待测样本
    - 初始化邻居数
    - 初始化距离列表【距离，标签】
    - 循环每个数据点，计算dis欧氏距离
    - 对dis按照距离排序，取前K个数据点
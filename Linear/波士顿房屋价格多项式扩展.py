"""
https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression
LinearRegression：线性回归。普通最小二乘线性回归。
    fit_intercept :
    布尔类型，初始为True。决定在这个模型中是否有intercept，即偏移量，即类似于线性函数y = w1x1 + w0 中的w0。 如果False则无。
    normalize:
    布尔类型，初始为False。如果fit_intercept设置为False，那这个参数就会被忽略。反之，设为True，则模型在回归之前，会对特征集X减去平均数并除以L2范式(没懂)，理解为一种标准化的规则。如果设为了False，而你又想标准化特征集合，则需要使用 sklearn.preprocessing.StandardScaler类来进行预处理。
    copy_X：
    布尔类型，初始化为True。True则，特征集合不变，反之会被复写。
    n_jobs:
    The number of jobs to use for the computation
    初始为None，表示用1个处理器计算；-1代表所有处理器，只用于多个目标集问题的提速和大型问题。
    coef_：权重矩阵，理解为线性函数y = w1x1 + w0 中的W1
    intercept_ :偏移量，理解为线性函数y = w1x1 + w0 中的W0
    rank_: 特征矩阵的秩
    singular_:特征矩阵的奇异值

https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso
Lasso：使用 L1 先验作为正则化器（又名 Lasso）训练的线性模型。

https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge
Ridge：具有 l2 正则化的线性最小二乘法。

https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html#sklearn.linear_model.ElasticNet
ElasticNet：以 L1 和 L2 先验相结合的线性回归作为正则化器。


"""
import sys

import pandas as pd
import joblib
import warnings

from matplotlib import pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression,Lasso,Ridge,ElasticNet

warnings.filterwarnings('ignore')

if __name__ == '__main__':
    # 加载数据
    data = pd.read_csv('../datas/boston_house_prices.csv', sep=',', header=None)
    # 获取特征属性
    x = data.iloc[:, :-1]
    y = data.iloc[:, -1]
    # 划分训练集和测试集
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=11)
    # 特征工程（多项式扩展）
    poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)
    poly.fit(x_train)
    x_train_poly = poly.transform(x_train)
    x_test_poly = poly.transform(x_test)
    joblib.dump(poly, '../models/poly.m')
    # 构建模型
    linear = LinearRegression(fit_intercept=True)
    # 训练模型
    linear.fit(x_train_poly, y_train)
    """
    权重矩阵:linear.coef_
    [-1.57434625e+01 -6.47256886e-02 -3.24763696e+00  5.02404131e+01
  8.73599060e+01  2.79933989e+01  1.28467434e+00  2.25979356e-01
  5.07998231e-02  9.45863754e-02  3.95761857e+00  7.68766637e-02
  2.27984035e+00  3.88343440e-01 -1.89157103e-01  2.78516954e+00
 -1.09801988e+00  2.43144161e-01  3.25662873e-04 -1.73547190e-01
 -6.58874007e-01  3.56369543e-02  5.01061310e-01 -5.02826590e-04
  2.94158261e-02 -8.79196891e-04 -3.98092342e-02 -3.81550937e-01
  1.96397366e-02  5.41751912e-04  8.75939246e-03 -2.45710591e-03
  5.56924884e-04 -3.58299545e-03 -2.68164826e-05 -9.50021176e-03
 -4.07596964e-01  3.42810858e+00  3.96938075e-01 -1.66613531e-03
 -4.20358038e-02  1.89040559e-02  6.02311642e-04 -6.30704613e-02
  3.62827885e-04  1.98601234e-02 -3.21121132e+01 -4.94481472e+00
 -7.94270308e-03 -8.02619252e-01 -7.50638222e-01  2.98389981e-02
 -1.64774376e-01  1.38268032e-02 -2.34798784e-01  6.11733987e+00
 -1.00856690e+00  4.31003823e+00  1.04382043e+00 -7.38555043e-02
 -5.78701652e+00 -2.63245151e-03  5.32511410e-01 -6.53984910e-02
  1.06451699e-01  2.11975250e-02 -3.01514862e-02 -5.33963108e-01
 -3.37784949e-03 -3.22650459e-01 -2.37229804e-02  1.74723700e-02
 -3.83367879e-04 -1.07111328e-03 -5.97031677e-04 -5.34146834e-03
  3.00005157e-02 -5.71256558e-03 -3.43373302e-02 -4.01871808e-03
  1.25347790e-01 -1.20519086e-03 -4.33637375e-02 -6.87049847e-04
 -1.53839647e-02  8.57450370e-03  3.72141448e-05 -1.74891585e-03
  4.27963105e-04 -2.02442607e-02 -2.98060015e-04]
    偏移量:linear.intercept_
    -199.1425021198433
    """
    # 预测测试集
    y_test_hat = linear.predict(x_test_poly)
    """
    [21.46042266 11.36026064 27.27962228 22.81720713  8.70661943 11.73870028
 20.95735623  0.6085596  42.2222806  27.09956515 22.765253   17.2477535
 24.80332783 20.39696582 13.25698506 13.78255561 33.48109514 28.97568554
 23.55346769 12.77511318 26.22982237 32.64167248 55.73324348 32.01291285
 10.90958359 11.44964077 16.27011082 18.12396716 26.58829193 22.51162669
  6.93860594  8.11475032 13.99346689 11.61721758 24.52193636 23.05374384
 25.1051518  18.26041197 20.01522845 32.07404705 22.71942095 27.69056789
 24.17748978  9.2819038  22.57986202 20.2181271  20.6344605  22.21477472
 16.96997081 17.7841707  25.03492369 23.42437137 22.9241797  35.11828515
 45.32790354 33.18517591 34.70688501 18.83564224 23.65699765 16.94367632
 18.65714228 27.22294787 14.10656158 23.96572686 20.65624834  5.61985423
 13.10794678 16.00184758 29.48830321 52.94089605 32.01102711 17.08421524
 17.2636022   6.82102715 14.33112354 27.11534802 35.98935265 15.12672665
 28.4618619  22.74886628 13.94250857 27.30214799 48.82755696 20.53830082
 18.9301621  14.90239139 17.08784281 31.84755636 15.11054968 11.49730079
 14.17748839 23.93493698 21.60275374 16.82963944 24.3961811  29.55892917
 22.96204126 24.24326712 16.0388663  18.8402327  14.52406425 16.30847935
 28.92393384 22.5915932  16.53046537 17.39224447 46.57647769 27.17870885
 24.82657911 17.55807255  7.77314149 22.15454925 24.8060112  18.86646457
 22.23202091 13.57348039 16.33676591 12.08877584 26.22770441 16.36484332
 13.69432042 25.45389899 20.55828985 27.36151078 22.79397154 19.501696
 26.09479851 24.47698749 16.57283709 21.97567865 30.81169163 18.09510766
 35.83455103 15.30362367 45.83436021 52.44445285 20.47078761 20.97597904
 26.64502192 21.17382423 34.81533263  6.85762701 32.42261197 22.04012431
 12.5006347   7.7657764  39.14063634 20.39145598 25.19150846 19.53691631
 43.18513322 24.01795889]
    """
    y_train_hat = linear.predict(x_train_poly)
    """
    [19.37651816 19.13296855 18.92517424 24.2817168  17.72133106 26.18273255
 26.65723551 36.75957659 23.31830773 28.0418609  23.19702703 24.23457903
 26.774672   18.69367814 26.35798442 42.96407841 10.93549427 35.47421826
 16.87146773 30.54327211 16.64459001 21.08708046 49.04928506 13.37813246
 22.74409221 24.15183558 18.17693715 27.47186511 14.79639885 19.32199146
 18.75231775 42.36851757 25.12358062 24.11040787  8.72286024 15.55291616
 19.01268676 15.66978529 35.5925865  17.63037675 26.23808371 30.21745378
 17.00286997 19.80708408 23.50789893 20.00467258 22.76994847 33.30266147
 13.21941661 30.91092486 33.91990401 33.62126054  0.09027147 21.45902755
 18.82045002 22.24748845 32.12940685 30.5128606  14.55239311 36.55764865
 21.41063309 47.45178485 19.60824749 17.49709152 21.33036451  6.8089718
 28.47889258 28.51024644 21.8960173   8.05753978 15.82317863 23.0020543
 12.60700752 34.64142216 20.08093172  7.80467041 44.66883638 23.16345371
 35.87760436 29.57875577 24.72778842 16.00380386 33.99878385 23.25885487
 28.47323383 15.23923319 30.28854517 21.38944879 12.47348058 19.94361392
 29.24386192 20.95844383 25.52031645 18.6483506  15.14155974 16.55086262
 49.32435872 25.42844368 21.05648577 14.3001179  23.40707937 16.29895839
 10.1817127  14.69122125 15.55310655 21.90713082 31.74619573 30.94560231
 23.52159755 13.69550059 15.66180382 22.75753235 16.90060416 21.19243525
 21.15566356 39.35450558 35.66936239 19.44352858 47.78756032 20.70857617
 25.22963978 44.12491695 26.1499791  13.11288131 22.33866814 22.27227793
 18.81440258 20.60778373 19.54270593 36.8511955  14.25612172 17.38455943
 41.25169705 11.04607504 31.29750875 18.39883249 23.34522871 18.24721475
 20.41697493 17.8636134  28.46407199 26.31015759 24.99520366 15.02528606
 16.90500536 27.26232198 20.24712019 24.58552682 42.73101143 12.98562186
 14.0059855  26.75311991 20.10820373 31.87544071 25.21036656 35.76552198
 13.61989789 13.39644345 14.79496907 19.12948605 20.37443306 25.18551831
 21.31146157 22.2647759  20.43345862 21.45219007 19.99687785 48.23194864
 19.7431072  45.30742507 18.21144292 29.13270846 27.9995579  31.86198096
 27.40568392 18.46829782 19.56121313 10.38455948 23.86632864 16.96240225
 15.51031713 39.60611157 21.33655689 17.15142042 12.44604538 22.34313556
 25.40317443 21.860571   17.68567823  9.36654237 14.19810964 15.01394924
 19.56968907 20.28798051 16.09694763 13.7156139  21.49475343 16.20399667
 22.34686715 46.88866835 19.55560041 13.4850091  16.9309931  24.03586106
 12.20332392 22.50791405  6.75664603  9.81226939 20.01292908 13.08957688
 23.53349546 22.73243017 28.06894178 17.02398745 11.66795893 22.88013827
 16.52597226 16.16181314  5.86909745 23.90433221 17.54333969 23.58046985
 39.53321389 24.31047473 21.35291285 25.48179269 23.12808555 20.79199625
 25.04592481 16.52138757 20.59406798 17.76887508 27.47417819 15.71852005
 22.0187068  20.78132057 23.63603784 15.53375261  5.39460081  9.17053176
 28.10625221 24.95529585 24.72246528 14.56636271 20.46241648 21.7546752
 17.60359914 16.30901628 21.28791447 13.73209776 19.65704758 20.14231392
 23.21107203 14.44778704 23.23302676 37.02537831 26.07481733 44.85627192
 18.46590614 20.25324788 23.19513166 12.75996481 18.62333473 28.44861375
 35.45782504 35.16264686 21.1776582  23.27155549 32.5111815  17.98041233
 19.19231025 16.20901514 25.56186546 36.30677104 21.35055845 25.73779621
 10.20567565 16.75189768 33.97846098 23.72251091 31.90100493 21.87932227
 19.80433526 18.63833223 24.85652223 12.20243165 22.533911   23.42982202
 29.12290875 33.56026576 30.39813489 22.37796743 26.08849401 15.18107273
 28.48980146 21.72333212 49.47001232 25.57218831 13.3980332  21.95153842
 18.30562898 34.0752454  13.93752803 16.57127078 27.52068077 15.54028527
 11.10517929 13.06388934 18.6557467  31.91766735 22.25968644 17.66028863
 13.09757711 24.09799116 20.90497788 18.62350966 32.92542092 22.56669273
 50.14792274 20.73258037 25.11760372 19.83880048 16.89937548 15.33883923
 18.14992071 25.75809192 13.30894628 35.43738684 21.94296535 19.12207852
  7.88036962 13.45116776 15.99372147  9.59169155 20.99329428 25.91492817
 25.35149458  6.48320107 45.38850343 23.79589934 26.08172846 18.45470012
 47.48505484 22.50439267 19.58490308 46.37631002 20.24190886 22.11403688
 11.27942183 19.45341506 25.34997707 28.68333838 27.01438809 18.87294229]
    """
    linear.score(x_train_poly, y_train)  # 0.9322112790115817
    linear.score(x_test_poly, y_test)  # 0.8721315260613942
    # 训练集 画图
    plt.figure(num="train")
    plt.plot(range(len(x_train)), y_train, 'r', label=u'true')
    plt.plot(range(len(x_train)), y_train_hat, 'g', label=u'predict')
    plt.legend(loc='upper right')
    plt.show()
    # 测试集 画图
    plt.figure(num="train")
    plt.plot(range(len(x_test)), y_test, 'r', label=u'true')
    plt.plot(range(len(x_test)), y_test_hat, 'g', label=u'predict')
    plt.legend(loc='upper right')
    plt.show()
